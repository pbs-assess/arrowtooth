---
title: "Arrowtooth Flounder (*Atheresthes stomias*) Stock Assessment for the West Coast of British Columbia in 2021"
french_title: "Évaluation du stock de la plie à dents de flèche (Atheresthes stomias) sur la côte ouest de la Colombie-Britannique en 2021 Colombie-Britannique en 2021"
author: |
  Chris J. Grandin^1^,
  Sean C. Anderson^1^ and
  Philina A. English^1^
author_list: "Grandin, C.J., Anderson, S.C. and English, P.A."
address: |
  ^1^Pacific Biological Station\
     Fisheries and Oceans Canada, 3190 Hammond Bay Road\
     Nanaimo, British Columbia, V9T 6N7, Canada
french_address: |
  ^1^Station bioslogique du Pacifique\
     Pêches et Océans Canada, 3190 Hammond Bay Road\
     Nanaimo, Colombie-Britannique, V9T 6N7, Canada\
month: September
french_month: Septembre
# Warning that these may not be used. It seems that the code
# looks at the `path` and `mcmcpath` list elements of the model
# to determine which path its in (somehow). Needs to be fixed
# so that these values supercede the ones in the RDS file
model_path: "/srv/arrowtooth/2022"
nongit_path: "/srv/arrowtooth/arrowtooth-nongit"
base_model_path: "/srv/arrowtooth/2022/01-base-models/01-base-model"
year: 2024
report_number: "071"
region: Pacific Region
french_region: "Région du Pacifique"
isbn: "978-0-660-38322-4"
cat_no: "Fs70-6/2021-012E-PDF"
citation_english: "Grandin, C.J., Anderson, S.C. and English, P.A. Arrowtooth Flounder (Atheresthes stomias) Stock Assessment for the West Coast of British Columbia in 2021. DFO Can. Sci. Advis. Sec. Res. Doc. 2024/071."
citation_french: "Grandin, C.J., Anderson, S.C. et English, P.A. Évaluation du stock de la plie à dents de flèche (Atheresthes stomias) sur la côte ouest de la Colombie-Britannique en 2021. DFO Secr. can. de consult. sci. du MPO. Doc. de rech 2024/071."
citation_other_language: "Will be injected by the preprocessor"

abstract: |
  Arrowtooth Flounder (*Atheresthes stomias*, Turbot) are an important component of the bottom trawl fishery in British Columbia. They are managed as a coastwide stock, with a current Total Allowable Catch (TAC) of 5 kilotonnes (kt) and catch of 3.051 kt in 2021. Prior to the introduction of freezer trawlers in the mid-2000s, most of the historical catch of Arrowtooth Flounder is understood to have been discarded at sea. This was largely due to proteolysis, which occurs in the muscle tissue of this species a short time after it is caught, making the flesh unpalatable. In the past decade, markets have been established for fillets that have been frozen at sea.

  This assessment fits a two-sex two-fleet Bayesian age-structured model to catch, survey, and age-composition data from the years 1996--2021 for management areas 3CD (West Coast Vancouver Island), 5AB (Queen Charlotte Sound), 5CD (Hecate Strait), and 5E (West Coast Haida Gwaii) combined. Catch data prior to the introduction of at-sea observers in 1996 were considered too unreliable for inclusion in the assessment due to unknown quantities of discarding at sea.

  The base model presented in this assessment estimates the 2022 median spawning biomass to be 67.77 kt and to have been on a decreasing trajectory since 2011, with a flattening trend from 2020-2022. Reference points based on maximum sustainable yield (MSY) were strongly impacted by the relationship between estimated maturity ogives and commercial age selectivity in the trawl fisheries. Reference points based on fractions of $B_0$ (unfished spawning biomass) were chosen instead, as was done in the last assessment. The median 2022 spawning biomass was projected to be below the USR (Upper Stock Reference) $0.4B_0$ and above the LRP (Limit Reference Point) $0.2B_0$. There was zero probability that the spawning biomass was below the LRP of $0.2B_0$ in 2022 in the base model, although one sensitivity model with the selectivity set to be time-varying for the `r knitr::load_cache('library-setup', 'qcsss')`, the relative biomass reached `r knitr::load_cache('library-setup', 'f(models$sens_grps[[4]][[3]]$mcmccalcs$depl_quants[,ncol(models$sens_grps[[4]][[3]]$mcmccalcs$depl_quants)][2], 2)')`. Other sensitivity analyses were done to test the effects of fixed parameters, prior probability distributions, and input data treatment on model outcomes. In several sensitivity models, there were poor MCMC (Markov chain Monte Carlo) diagnostics or unreasonable estimates of selectivity and/or catchability. A series of retrospective model runs back eight years indicated a distinct change in the biomass estimates for the model. Prior to 2019, retrospective models had a more optimistic view of the stock, with a terminal year relative biomass of approximately 0.5 or greater. After 2019, models estimated a terminal year relative biomass of approximately 0.4 or less.

  Management advice is provided in the form of a harvest decision table that forecast the impacts of a range of 2022 catch levels on Arrowtooth Flounder stock status relative to the reference points. The base-model decision table suggests that a 2022 catch equal to 5 kt (the 2022 TAC), would result in a 2023 biomass being below the USR of $0.4B_0$ with a probability of $0.68$. The same catch would give a near zero probability of the 2023 biomass falling below the LRP of $0.2B_0$. A constant catch equal to 15 kt would result in a 2026 biomass with an approximate $0.5$ probability of being below the $0.2B_0$ LRP. A reference removal rate of $U_{0.4B_0} = 10.5$% of the vulnerable population annually, which is equivalent to an annual removal of approximately 4.4 kt, was estimated to take the stock to $0.4B_0$ in the long term (50 years) assuming that the low recruitment estimated from 2010 to 2019 continues.
  
  The size of catches and discards prior to 1996, the lack of random-stratified surveys prior to 2005 that together cover the entire coast, the estimation of maturity and selectivity curves, the assumed magnitude of recruitment variability, and the estimation of $B_0$ are major sources of uncertainty in this assessment that make it challenging to estimate the size and productivity of the stock. The use of a stitched geostatistical survey to replace the separate synoptic survey indices could help resolve some issues fitting the Queen Charlotte Sound Synoptic survey index, which has a lower rate of decline than the other survey indices. After evaluating ecosystem considerations and known biology of the stock, there are no clear indications that current environmental conditions should modify the catch advice in this assessment. Given the stock is estimated slightly below the USR in the base model and close to the LRP under one sensitivity model with higher recruitment variation, as well as declining estimated spawning stock biomass, declining survey indices, and declining estimated recruitment, it is suggested that this stock assessment be updated with new data in approximately two years when one additional survey has been run in each area of the coast.

french_abstract: |
  La limande à dents de flèche (*Atheresthes stomias*, turbot) est un élément important de la pêche au chalut de fond en Colombie-Britannique. Elle est gérée comme un stock côtier, avec un total admissible des captures (TAC) actuel de 5 kilotonnes (kt) et des captures de 3,051 kt en 2021. Avant l'introduction des chalutiers congélateurs au milieu des années 2000, la plupart des captures historiques de limande à dents de flèche étaient rejetées à la mer. Cela était dû en grande partie à la protéolyse, qui se produit dans le tissu musculaire de cette espèce peu de temps après la capture et qui rend la chair peu appétissante. Au cours de la dernière décennie, des marchés ont été créés pour les filets qui ont été congelés en mer.

  Cette évaluation adapte un modèle bayésien structuré par âge à deux sexes et deux flottes aux données de capture, d'enquête et de composition par âge des années 1996--2021 pour les zones de gestion 3CD (côte ouest de l'île de Vancouver), 5AB (détroit de la Reine-Charlotte), 5CD (détroit d'Hécate), et 5E (côte ouest de Haida Gwaii) combinées. Les données de capture antérieures à l'introduction des observateurs en mer en 1996 ont été jugées trop peu fiables pour être incluses dans l'évaluation en raison des quantités inconnues de rejets en mer.

  Le modèle de base présenté dans cette évaluation estime que la biomasse féconde médiane de 2022 est de 67,77 kt et qu'elle suit une trajectoire décroissante depuis 2011, avec une tendance à l'aplatissement à partir de 2020-2022. Les points de référence basés sur le rendement maximal durable (RMD) ont été fortement influencés par la relation entre les ogives de maturité estimées et la sélectivité commerciale par âge dans les pêcheries au chalut. Des points de référence basés sur des fractions de $B_0$ (biomasse féconde non pêchée) ont été choisis à la place, comme cela avait été fait dans la dernière évaluation. La biomasse féconde médiane de 2022 devrait être inférieure à l'USR (Upper Stock Reference) $0,4B_0$ et supérieure au LRP (Limit Reference Point) $0,2B_0$. La probabilité que la biomasse reproductrice soit inférieure au PRL $0,2B_0$ en 2022 était nulle dans le modèle de base, bien qu'un modèle de sensibilité avec une plus grande variance du recrutement ait estimé que le stock était proche du PRL. D'autres analyses de sensibilité ont été effectuées pour tester les effets des paramètres fixes, des distributions de probabilité a priori et du traitement des données d'entrée sur les résultats du modèle. Dans plusieurs modèles de sensibilité, les diagnostics MCMC (Markov chain Monte Carlo) étaient médiocres ou les estimations de la sélectivité et/ou de la capturabilité étaient déraisonnables. Une série d'exécutions rétrospectives du modèle sur huit ans a indiqué un point de rupture distinct lorsque les données de 2019 ont été ajoutées. Depuis 2019, les données entraînent des baisses de la biomasse reproductrice estimée entre 2010 et 2019.

   L'avis de gestion est fourni sous la forme d'une table de décision sur les captures qui prévoit les impacts d'une gamme de niveaux de capture en 2022 sur l'état du stock de flétan noir par rapport aux points de référence. La table de décision du modèle de base suggère qu'une prise de 2022 égale à 5 kt (le TAC de 2022) entraînerait une biomasse de 2023 inférieure à l'USR de $0,4B_0$ avec une probabilité de $0,68$. La même prise donnerait une probabilité quasi nulle que la biomasse de 2023 soit inférieure au PRL de $0,2B_0$. Une prise constante égale à 15 kt donnerait une biomasse en 2026 avec une probabilité d'environ $0,5$ d'être inférieure au PRL de $0,2B_0$. Un taux de prélèvement de référence de $U_{0,4B_0}$ = 10,5% de la population vulnérable chaque année, ce qui équivaut à un prélèvement annuel d'environ 4,4 kt, a été estimé pour amener le stock à $0,4B_0$ à long terme, en supposant que le faible recrutement estimé de 2010 à 2019 se poursuive.

  La taille des captures et des rejets avant 1996, l'absence de campagnes stratifiées aléatoires avant 2005 couvrant l'ensemble de la côte, l'estimation des courbes de maturité et de sélectivité, l'ampleur supposée de la variabilité du recrutement et l'estimation de $B_0$ sont des sources majeures d'incertitude dans cette évaluation qui rendent difficile l'estimation de la taille et de la productivité du stock. L'utilisation d'un relevé géostatistique assemblé pour remplacer les indices des relevés synoptiques distincts pourrait aider à résoudre certains problèmes liés à l'indice du relevé synoptique du détroit de la Reine-Charlotte, dont le taux de déclin est plus faible que celui des autres indices du relevé. Après avoir évalué les considérations relatives à l'écosystème et la biologie connue du stock, rien n'indique clairement que les conditions environnementales actuelles devraient modifier l'avis sur les captures dans la présente évaluation. Étant donné que le stock est estimé légèrement en dessous de l'USR dans le modèle de base et proche du LRP dans un modèle de sensibilité avec une plus grande variation du recrutement, ainsi qu'une biomasse estimée du stock reproducteur en déclin, des indices de prospection en déclin et un recrutement estimé en déclin, il est suggéré que cette évaluation du stock soit mise à jour avec de nouvelles données dans environ deux ans, lorsqu'une prospection supplémentaire aura été réalisée dans chaque zone de la côte.

header: "" # "Draft working paper --- Do not cite or circulate"
# `show_continued_text` is a logical which, if `true`, places
# "Continued on the next page..." and "...Continued from the previous page" or
# the french equivalents (if `french` = `true`) on all long tables created
# with `csas_table()` that cross page boundaries. If `false`, these will
# both be absent from all tables. If it is missing or any other value than
# `true` or `false`, it will be assumed to be `true`
show_continued_text: false
output:
 csasdown::resdoc_pdf:
   # build_rds is a toggle to re-build all the RDS files for the models
   build_rds: false
   keep_md: true
   # `lualatex` is required for `accessibile_pdf` to work
   latex_engine: lualatex
   # If `true`, alternative figure text and figure tags are added for
   # PDF web accessibility compliance
   accessible_pdf: false
   # The name of the directory containing pre-made figures such as png files
   # that will be included using `include_graphics()`
   figures_dir: figure
   # This value will be the return value for `fr()` in your code
   french: false
   # copy_sty is a toggle to copy the style file from the csasdown package
   # every time you compile the document. If false, any changes you have
   # made to the style file in your project will remain between
   # compilations. If true, your changes will be lost when you compile
   copy_sty: true
   # line_nums is a toggle to show line numbers on the left side of the page
   line_nums: false
   # line_nums_mod represents showing every Nth line if line_nums is true
   line_nums_mod: 1
   # lot_lof is a toggle to show/not show the lists of tables and figures
   # at the beginning of the document
   lot_lof: false
   # draft_watermark is a toggle to show/not show a DRAFT watermark across
   # every page
   draft_watermark: false
   # include_section_nums, if true includes section numbering in the
   # document body, if false, no numbering in the document body but the
   # TOC will still show numbering
   include_section_nums: true
   # highlight is the theme to use for code output. Must be one of the
   # list given by:
   # pandoc --list-highlight-styles
   # which are:
   # pygments, tango, espresso, zenburn, kate, monochrome, breezedark,
   # haddock or the name of a custom *.latex file which is most easily
   # made by copying one from  the csasdown library 'themes' directory,
   # this directory on your machine:
   # file.path(.libPaths(), "csasdown", "themes")
   # to your working directory (the one containing index.Rmd)
   # To change the foreground text color, change the RGB value in the
   # line containing 'DefineVerbatimEnvironment'
   # To change background color, change the RGB values in the line
   # containing 'shadecolor'
   highlight: tango
# ------------
# End of options to set
knit: (function(input, ...) {
       csasdown::render('_bookdown.yml')
      })
link-citations: true
bibliography: bib/refs.bib
# Any extra LaTeX code for the header:
header-includes:
  - \usepackage{amsmath}
---

```{r setup, echo = FALSE, cache = FALSE, message = FALSE, results = "hide", warning = FALSE}

curr_dir <- basename(getwd())
parent_dir <- basename(dirname(getwd()))
if(curr_dir != "doc" && parent_dir != "arrowtooth"){
  stop("You must be in the 'arrowtooth/doc' directory to source this file.\n",
       "The current directory is: ", getwd(),
       call. = FALSE)
}

library(knitr)
if (is_latex_output()) {
  knitr_figs_dir <- "knitr-figs-pdf/"
  knitr_cache_dir <- "knitr-cache-pdf/"
} else {
  knitr_figs_dir <- "knitr-figs-docx/"
  knitr_cache_dir <- "knitr-cache-docx/"
}
fig_asp <- 0.618
fig_width <- 8
fig_out_width <- "5.5in"

user <- Sys.info()[["user"]]
opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  results = 'hide',
  comment = "#>",
  fig.path = knitr_figs_dir,
  cache.path = knitr_cache_dir,
  fig.asp = fig_asp,
  fig.width = fig_width,
  out.width = fig_out_width,
  echo = FALSE,
  # autodep = isTRUE(user %in% "seananderson"),
  # cache = isTRUE(user %in% "seananderson"),
  cache.comments = FALSE,
  # These two lines needed for the maps with geom_sf() to be rendered
  # correctly
  dev = "ragg_png",
  fig.ext = "png",
  dpi = 180,
  fig.align = "center",
  fig.pos = "H")
options(
  # Prevent xtable from adding a timestamp comment to the table code
  # it produces
  xtable.comment = FALSE,
  # Don't allow kableExtra to load packages, we add them manually in
  # csasdown
  kableExtra.latex.load_packages = FALSE,
  # Stop chunk output (echo) running into the margins
  width = 80,
  # Don't use scientific notation (stops tables from showing 1.2e3, etc.)
  scipen = 999)
# Fixes weird bug where knitr::include_graphics() thinks the non-git folder
# is relative
options(knitr.graphics.rel_path = FALSE)
#meta <- rmarkdown::metadata$output
#options(french = meta$`csasdown::resdoc_pdf`$french)
```

```{r library-setup, cache = TRUE, fig.keep = 'none'}
# Libraries in alphabetical order

library(devtools)
library(dplyr)
if(as.logical(length(grep("grandin", user)))){
  load_all("~/github/pbs-assess/gfiscamutils")
  load_all("~/github/pbs-assess/gfutilities")
  load_all("~/github/pbs-assess/gfplot")
  load_all("~/github/pbs-assess/csasdown")
  load_all("~/github/pbs-assess/rosettafish")
load_all("~/github/pbs-assess/arrowtooth")
}else if(user == "seananderson"){
  load_all("~/src/gfutilities/")
  load_all("~/src/gfiscamutils/")
  load_all("~/src/csasdown/")
  load_all("~/src/rosettafish/")
  load_all("~/src/arrowtooth")
  library(gfplot)
}else{
  library(gfiscamutils)
  library(gfutilities)
  library(gfplot)
  library(csasdown)
}
library(ggplot2)
library(gridExtra)
library(here)
library(kableExtra)
library(purrr)
library(tidylog, warn.conflicts = FALSE)

meta <- rmarkdown::metadata$output
build_rds <- FALSE
if(!is.null(meta)){
  build_rds <- meta$`csasdown::resdoc_pdf`$build_rds
}
```

```{r include = FALSE}

source(here::here("doc/model-names.R"),
       local = knitr::knit_global())
source(here::here("doc/load-models.R"),
       local = knitr::knit_global())
```

```{r data-setup, cache.lazy = FALSE}
# This chunk requires the chunk above that loads load-models.R has been run

bc <- tr("British Columbia")
sp <- tr("Arrowtooth Flounder")
iscam <- "ISCAM"

month_fishing_starts <- 2
day_fishing_starts <- 21

data_dir <- file.path(drs$nongit_dir, "data")
data_output_dir <- file.path(drs$nongit_dir, "data-output")

if(!dir.exists(data_dir)){
  stop("Data directory does not exist: ", data_dir, call. = FALSE)
}
iphc_file <- file.path(data_dir, "iphc-survey-index.rds")
if(!file.exists(iphc_file)){
  stop("IPHC file does not exist: ", iphc_file, call. = FALSE)
}
discard_cpue_file <- file.path(data_dir,
"cpue-predictions-arrowtooth-flounder-modern-3CD5ABCDE-discard-july-26-feb-fishing-year.csv")
#"cpue-predictions-arrowtooth-flounder-modern-3CD5ABCDE-discard-july-26-jan-1-year.csv")
if(!file.exists(discard_cpue_file)){
  stop("Discard CPUE file does not exist: ", discard_cpue_file, call. = FALSE)
}
stitched_syn_file <- file.path(data_dir, "stitched-syn-index.rds")
if(!file.exists(stitched_syn_file)){
  stop("Stitched Synoptics file does not exist: ", stitched_syn_file, call. = FALSE)
}

iphc <- readRDS(iphc_file)$series_ABCD_full$ser_longest
discard_cpue <- read_csv(discard_cpue_file)
stitched_syn <- readRDS(stitched_syn_file)

dat <- readRDS(file.path(drs$nongit_dir, "data",
                         "arrowtooth-flounder-aug11-2022.rds"))

# Remove 2014 WCHG index point
wchg_2014_row <- 
  dat$survey_index$survey_abbrev == "SYN WCHG" & dat$survey_index$year == 2014
if(any(wchg_2014_row)){
  dat$survey_index <- dat$survey_index[-which(wchg_2014_row), ]
}

# These must be removed for call to add_extra_indices()
survey_index <- dat$survey_index |> 
  select(-species_common_name, -species_science_name)
survey_index <- add_extra_indices(survey_index, 
                                  iphc = iphc,
                                  discard_cpue = discard_cpue,
                                  stitched_syn = stitched_syn)
# Survey index for geostat
geo_files <- dir(file.path(drs$nongit_dir, "survey-geostat"),
                 full.names = TRUE,
                 pattern = "^i-arrowtooth.*\\.rds$")
ind_geo <- purrr::map_dfr(geo_files, readRDS)
survey_index_geo <- ind_geo |>  filter(model == "no-depth") |> as_tibble()
survey_index_geo <- survey_index_geo |>
  rename(biomass = est,
         lowerci = lwr,
         upperci = upr,
         re = se,
         survey_abbrev = surveys) |>
  mutate(num_sets = NA,
         num_pos_sets = NA,
         survey_series_id = NA,
         survey_series_desc = "") |>
  select(-c(log_est, species, survey, ssids, ssid_string,
            family, anisotropy, spatiotemporal, share_range,
            model, max_grad)) |>
  select(year, biomass, lowerci, upperci,
         re, num_sets, num_pos_sets,
         survey_series_id, survey_abbrev, survey_series_desc) |>
  add_extra_indices(discard_cpue = discard_cpue)
# Add HS MSA survey so plot will work
hs_multi <- survey_index |> filter(survey_abbrev == "OTHER HS MSA")
survey_index_geo <- survey_index_geo |>
  bind_rows(hs_multi)

# Areas 3CD and 5ABCDE only 
major_areas <- c("03","04", "05", "06", "07", "08", "09")
tidy_areas <- c("3[CD]+", "5[ABCDE]+")
survey_sets <- dat$survey_sets |> 
  filter(major_stat_area_code %in% major_areas)
survey_samples <- dat$survey_samples |> 
  filter(major_stat_area_code %in% major_areas)
survey_samples_syn <- survey_samples |> 
  filter(survey_abbrev %in% c("SYN QCS",
                              "SYN HS",
                              "SYN WCVI",
                              "SYN WCHG"))
commercial_samples <- dat$commercial_samples |> 
  filter(major_stat_area_code %in% major_areas)
comm_ft <- extract_fleet_samples(commercial_samples)
comm_ss <- extract_fleet_samples(commercial_samples, include = FALSE)

# Aggregated commercial catch
month_start <- 2
day_start <- 21
dat$catch <- dat$catch |> filter(year < 2022)
catch <- tidy_catch(dat$catch,
                    areas = tidy_areas,
                    month_fishing_starts = month_start,
                    day_fishing_starts = day_start)
# Catch by fleet
catch_ft <- extract_fleet_catch(dat$catch) |> 
  tidy_catch(areas = tidy_areas,
             month_fishing_starts = month_start,
             day_fishing_starts = day_start)

catch_ss <- extract_fleet_catch(dat$catch, include = FALSE) |> 
  tidy_catch(areas = tidy_areas,
             month_fishing_starts = month_start,
             day_fishing_starts = day_start)

cpue_spatial <- dat$cpue_spatial
cpue_spatial_ll <- dat$cpue_spatial_ll
age_precision <- dat$age_precision

theme_set(gfiscam_theme())

if(!exists("curr_gear_language")){
  curr_gear_language <- "en"
}

models <- set_gear_language(models, curr_gear_language)

base_model <- models$base_grps[[1]][[1]]
curr_gear_language <- ifelse(fr(), "fr", "en")

base_all_gears <- gear_lu_table(base_model, "all")
base_age_gears <- gear_lu_table(base_model, "age")
base_index_gears <- gear_lu_table(base_model, "index")
base_fleet_gears <- gear_lu_table(base_model, "fleet")

mcmc_chain_length <- 10000000
mcmc_num_samples <- 2000
mcmc_sample_freq <- mcmc_chain_length / mcmc_num_samples
mcmc_burn_in <- 1000
mcmc_actual_samples <- mcmc_num_samples - mcmc_burn_in

qcs <- tr("Queen Charlotte Sound Synoptic Survey")
hsmas <- tr("Hecate Strait Multispecies Assemblage Survey")
hss <- tr("Hecate Strait Synoptic Survey")
wcvis <- tr("West Coast Vancouver Island Synoptic Survey")
wchgs <- tr("West Coast Haida Gwaii Synoptic Survey")
dcpue <- tr("Discard CPUE Index")

la <- ifelse(fr(), "Évaluation de 2015", "2015 assessment")

# Number of parameters estimated (from PAR file)
num_params <- get_num_params_est(base_model)

# Catch table
ct <- as_tibble(base_model$dat$catch)
ct_start_yr <- min(ct$year)
ct_end_yr <- max(ct$year)

# Reference points (table values)
ref_pts <- as_tibble(base_model$mcmccalcs$params_quants)

# Projected biomass
end_yr <- base_model$dat$end.yr
assess_yr <- end_yr + 1
proj_yr <- assess_yr + 1
sbt_quants <- as_tibble(base_model$mcmccalcs$sbt_quants)
proj_bio <- sbt_quants[, as.character(assess_yr)] |> pull()

# Fishing mortality
f_max_by_gear <- map_dbl(base_model$mcmccalcs$ft_quants, ~{
  max(.x[2,])
})
f_max <- max(f_max_by_gear)
which_f_max <- which(f_max == f_max_by_gear)
which_f_max_gear <- base_model$dat$fleet_gear_names[which_f_max]
which_f_max_yr <- names(which(base_model$mcmccalcs$ft_quants[[which_f_max]][2, ] == f_max))

f_ci <- base_model$mcmccalcs$ft_quants[[which_f_max]][, base_model$mcmccalcs$ft_quants[[which_f_max]][2, ] == f_max]

# Relative spawning biomass
depl_end <- as_tibble(base_model$mcmccalcs$depl_quants) |>
  select(!!sym(as.character(assess_yr))) |> 
  pull()

# Columns of table_dec are:
# Catch, P(B2023<0.2B0), P(B2024<0.2B0), P(B2025<0.2B0), P(B2023<0.2B0),
# P(B2023<0.4B0), P(B2024<0.4B0), P(B2025<0.4B0), P(B2023<0.4B0),
# P(B2023<B2022), P(B2024<B2023), P(B2025<B2024)
table_dec <- table_decisions(base_model, ret_df = TRUE, digits = 3)
catch_col <- sym(grep(tr("Catch"), names(table_dec), value = TRUE))

# Decision table column indices for P(B_2023<B_2022)
i_2023 <- grep("\\{2023\\} < B", names(table_dec))
i_2024 <- grep("\\{2024\\} < B", names(table_dec))
i_2025 <- grep("\\{2025\\} < B", names(table_dec))
i_2026 <- grep("\\{2026\\} < B", names(table_dec))

prob_2023_2022_catch_0 <- table_dec |>
  filter(!!catch_col == 0) |> pull(i_2023)
prob_2023_2022_catch_5 <- table_dec |>
  filter(!!catch_col == 5) |> pull(i_2023)
prob_2023_2022_catch_10 <- table_dec |>
  filter(!!catch_col == 10) |> pull(i_2023)
prob_2023_2022_catch_15 <- table_dec |>
  filter(!!catch_col == 15) |> pull(i_2023)
prob_2024_2023_catch_10 <- table_dec |>
  filter(!!catch_col == 10) |> pull(i_2024)
prob_2025_2024_catch_10 <- table_dec |>
  filter(!!catch_col == 10) |> pull(i_2025)
prob_2026_2025_catch_10 <- table_dec |>
  filter(!!catch_col == 10) |> pull(i_2026)

probs_proj_less_assess <- table_dec |> select(i_2023) |> pull()
which_prob_less_50_50 <- which(probs_proj_less_assess < 0.5)
which_prob_less_50_50 <- which_prob_less_50_50[length(which_prob_less_50_50)]
prob_less_50_50 <- table_dec[which_prob_less_50_50, ]
prob_greater_50_50 <- table_dec[which_prob_less_50_50 + 1, ]

catch_less_50_50 <- prob_less_50_50 |> pull(catch_col)
val_less_50_50 <- prob_less_50_50 |> pull(i_2023)

# Decision table column indices for P(B_2023<0.4B0) and P(B_2023<0.4B0)
i_2023_4bo <- grep("\\{2023\\} < 0.4B", names(table_dec))
i_2023_2bo <- grep("\\{2023\\} < 0.2B", names(table_dec))

below_04bo <- table_dec |> 
  pull(i_2023_4bo)
range_below_04bo <- c(min(below_04bo), max(below_04bo))

below_02bo <- table_dec |> 
  pull(i_2023_2bo)
range_below_02bo <- c(min(below_02bo), max(below_02bo))

# For decision table caption text
prob_10t_below_2bo <- table_dec |> 
    filter(!!catch_col == 10) |> pull(i_2023_2bo)
prob_10t_below_4bo <- table_dec |> 
    filter(!!catch_col == 10) |> pull(i_2023_4bo)

# For the Projections and Decision table section
uncert_2025_catch_10 <- base_model$mcmc$proj |>
  filter(TAC == 10) |>
  pull(B2025)
uncert_2025_catch_10 <- quantile(uncert_2025_catch_10 / base_model$mcmc$params$sbo,
                                 probs = c(0.025, 0.5, 0.975))

lo_uncert_2025_catch_10 <- uncert_2025_catch_10[1]
hi_uncert_2025_catch_10 <- uncert_2025_catch_10[3]
diff_uncert_2025_catch_10 <- hi_uncert_2025_catch_10 - lo_uncert_2025_catch_10
names(diff_uncert_2025_catch_10) <- "difference"

```

```{r biological-params}

find_length_outliers <- function(xx) {
  yy <- stats::pnorm(xx,
    mean = mean(xx, na.rm = TRUE),
    sd = stats::sd(xx, na.rm = TRUE), log.p = TRUE
  )
  zz <- stats::qnorm(yy, log.p = TRUE)
  out <- zz[zz > 4 & !is.na(zz)]
  if (length(out) > 1L) {
    return(xx[which(zz > 4)])
  } else {
    return(numeric(0))
  }
}

length_samples_survey <- filter(
  dat$survey_samples,
  !length %in% find_length_outliers(dat$survey_samples$length)
)

length_samples_ft <- filter(
  comm_ft,
  !length %in% find_length_outliers(comm_ft$length)
)

length_samples_ss <- filter(
  comm_ss,
  !length %in% find_length_outliers(comm_ss$length)
)

all_length_samples <- bind_rows(length_samples_survey,
                                length_samples_ft,
                                length_samples_ss)

all_age_samples <- bind_rows(dat$survey_samples, comm_ft, comm_ss) |>
  filter(!is.na(age) & age < 40)

# Use function from this package as it is (very) slightly different than what 
# fit_mat_ogive() returns, and is what is input into the model
mat_fit <- export_mat_lw_age(dat$survey_samples, write_file = FALSE)
# TODO: what random effects wanted? If year, than params are saved as
# mat_fit$mat_perc$mean$f.mean.p0.5 and mat_fit$mat_perc$mean$m.mean.p0.5
# instead of mat_fit$mat_perc$f.p0.5 and mat_fit$mat_perc$m.p0.5. I assume
# fig:fig-mat should also be made to match 

# Natural mortality values in the control file
param_ctl_table <- models$bridge_grps[[3]][[2]]$ctl$params |>
  as_tibble(rownames = "param")
male_m_ctl <- exp(param_ctl_table |> filter(param == "log_m_male") |>
                    pull(ival))
female_m_ctl <- exp(param_ctl_table |> filter(param == "log_m_female") |>
                      pull(ival))
```

```{r proportion-female}

if(!exists("data_dir")){
  stop("`data_dir` does not exist. If running from command line, ",
       "source('index.Rmd') to set up all project variables", call. = FALSE)
}

prop_female_fn <- file.path(data_dir, "prop_female_output.rds")
if(file.exists(prop_female_fn)){
  prop_female_lst <- readRDS(prop_female_fn)
}else{
  comm_prop <- props_comm(dat$commercial_samples)
  surv_prop <- props_surv(surv_series = c(1, 3, 4, 16),
                          surv_series_names = c("qcsss",
                                                "hsss",
                                                "wcviss",
                                                "wchgss"),
                          surv_samples = dat$survey_samples,
                          surv_sets = dat$survey_sets)
  prop_female_lst <- list(comm_prop, surv_prop)
  saveRDS(prop_female_lst, prop_female_fn)
}

prop_female_means <- table_prop_female(prop_female_lst,
                                       ret_means = TRUE)

total_prop_female <- f(mean(prop_female_means), 2)
```

```{r model-param-value-calcs}

base_sbo <- get_parvals(base_model, "sbo")
base_bo <- get_parvals(base_model, "bo")
base_sbt <- get_parvals(base_model, "sbt")
base_depl <- get_parvals(base_model, "depl", digits = 2)
base_m_male <- get_parvals(base_model, "m_male", digits = 2)
base_m_female <- get_parvals(base_model, "m_female", digits = 2)
base_h <- get_parvals(base_model, "h", digits = 2)

bvals <- get_group_parvals(models$bridge_grps)
svals <- get_group_parvals(models$sens_grps)

# Extract parameter values from the table found in the control file
#
# @param model The iSCAM model
# @param param The parameter name (row)
# @param value The value (column)
# @param digits The number of decimal points to return
#
# @return The value or row
# @export
get_ctl_params <- function(model, param = NULL, value = NULL, ...){
  inp_params <- as_tibble(rownames_to_column(as.data.frame(model$ctl$params),
                                             var = "param"))

  if(!is.null(param)){
    inp_params <- filter(inp_params, param == !!param)
  }
  if(!is.null(value)){
    return(pull(inp_params, value))
  }
  inp_params
}

get_param_est <- function(model, param = NULL, est_digits = 2, ...){
  if(is.null(param)){
    stop("Must provode `param` name", call. = FALSE)
  }
  
  if(param == "log_m_female"){
    param ="m_sex1"
  }
  if(param == "log_m_male"){
    param ="m_sex2"
  }
  raw <- as_tibble(model$mcmccalcs$params_quants)[[param]]
  paste0(f(raw[2], est_digits), " (", f(raw[1], est_digits), "--", f(raw[3], est_digits), ")")
}

get_param_vals <- function(model, param, est = TRUE, ...){
  out <- NULL
  out$init <- get_ctl_params(model, param, "ival", ...)
  out$p1 <- get_ctl_params(model, param, "p1", ...)
  out$p2 <- get_ctl_params(model, param, "p2", ...)
  if(est){
    out$est <- get_param_est(model, param, ...)
  }
  out
}

base_vartheta <- get_param_vals(base_model, "vartheta", est = FALSE, digits = 5)
base_rho <- get_param_vals(base_model, "rho", est = FALSE, digits = 5)
base_sig_tau <- calc_sig_tau(get_param_vals(base_model, "rho", est = FALSE)$init,
                             get_param_vals(base_model, "vartheta", est = FALSE)$init)
base_sig <- f(base_sig_tau[1], 1)
base_tau <- f(base_sig_tau[2], 1)

base_h <- get_param_vals(base_model, "h")
base_h_prior1 <- calc_beta_mean_cv(base_h$p1, base_h$p2)[1]
base_h_prior2 <- calc_beta_mean_cv(base_h$p1, base_h$p2)[2]
base_h_prior_params <- paste(f(base_h_prior1, 2),
                             ",",
                             f(base_h_prior2, 2))
base_m_female <- get_param_vals(base_model, "log_m_female")
base_m_male <- get_param_vals(base_model, "log_m_male")

sens_1_2_vartheta <- get_param_vals(models$sens_grps[[1]][[2]], "vartheta")
sens_1_2_rho <- get_param_vals(models$sens_grps[[1]][[2]], "rho", est = FALSE)
sens_1_2_sig_tau <- calc_sig_tau(get_param_vals(models$sens_grps[[1]][[2]],
                                                           "rho", est = FALSE)$init,
                                 get_param_vals(models$sens_grps[[1]][[2]],
                                                           "vartheta")$init)
sens_1_2_sig <- f(sens_1_2_sig_tau[1], 3)
sens_1_2_tau <- f(sens_1_2_sig_tau[2], 1)
sens_1_2_h <- get_param_vals(models$sens_grps[[1]][[2]], "h")
sens_1_2_sbo <- get_param_vals(models$sens_grps[[1]][[2]], "sbo", est_digits = 0)

sens_1_3_vartheta <- get_param_vals(models$sens_grps[[1]][[3]], "vartheta")
sens_1_3_rho <- get_param_vals(models$sens_grps[[1]][[3]], "rho", est = FALSE)
sens_1_3_sig_tau <- calc_sig_tau(get_param_vals(models$sens_grps[[1]][[3]],
                                                           "rho", est = FALSE)$init,
                                 get_param_vals(models$sens_grps[[1]][[3]],
                                                           "vartheta")$init)
sens_1_3_sig <- f(sens_1_3_sig_tau[1], 1)
sens_1_3_tau <- f(sens_1_3_sig_tau[2], 1)
sens_1_3_sbo <- get_param_vals(models$sens_grps[[1]][[3]], "sbo", est_digits = 0)


sens_1_4_vartheta <- get_param_vals(models$sens_grps[[1]][[4]], "vartheta")
sens_1_4_rho <- get_param_vals(models$sens_grps[[1]][[4]], "rho", est = FALSE)
sens_1_4_sig_tau <- calc_sig_tau(get_param_vals(models$sens_grps[[1]][[4]],
                                                           "rho", est = FALSE)$init,
                                 get_param_vals(models$sens_grps[[1]][[4]],
                                                           "vartheta")$init)
sens_1_4_sbo <- get_param_vals(models$sens_grps[[1]][[4]], "sbo")

sens_1_4_sig <- f(sens_1_4_sig_tau[1], 1)
sens_1_4_tau <- f(sens_1_4_sig_tau[2], 1)

sens_1_5_h <- get_param_vals(models$sens_grps[[1]][[5]], "h")
sens_1_5_h_prior1 <- calc_beta_mean_cv(sens_1_5_h$p1, sens_1_5_h$p2)[1]
sens_1_5_h_prior2 <- calc_beta_mean_cv(sens_1_5_h$p1, sens_1_5_h$p2)[2]
sens_1_5_h_prior_params <- paste(f(sens_1_5_h_prior1, 2),
                             ",",
                             f(sens_1_5_h_prior2, 2))

sens_2_2_m_female <- get_param_vals(models$sens_grps[[2]][[2]], "log_m_female")
sens_2_3_m_female <- get_param_vals(models$sens_grps[[2]][[3]], "log_m_female")
sens_2_4_m_male <- get_param_vals(models$sens_grps[[2]][[4]], "log_m_male")
sens_2_5_m_male <- get_param_vals(models$sens_grps[[2]][[5]], "log_m_male")

# qk priors and estimates
base_qk_inp_params <- base_model$ctl$surv.q
base_qk_mean <- exp(base_qk_inp_params[rownames(base_qk_inp_params) == "priormeanlog"])[1]
base_qk_sd <- base_qk_inp_params[rownames(base_qk_inp_params) == "priorsd"][1]

sens_qk_inp_params <- models$sens_grps[[3]][[2]]$ctl$surv.q
sens_qk_mean <- exp(sens_qk_inp_params[rownames(sens_qk_inp_params) == "priormeanlog"])[1]
sens_qk_sd <- sens_qk_inp_params[rownames(sens_qk_inp_params) == "priorsd"][1]

sens_qkp_inp_params <- models$sens_grps[[3]][[3]]$ctl$surv.q
sens_qkp_mean <- exp(sens_qkp_inp_params[rownames(sens_qkp_inp_params) == "priormeanlog"])[1]
sens_qkp_sd <- sens_qkp_inp_params[rownames(sens_qkp_inp_params) == "priorsd"][1]

sens_3_2_selex_f_qcs <- filter(models$sens_grps[[3]][[2]]$mcmccalcs$selest_quants, gear == "QCS Synoptic", sex == 2)$a_hat
sens_3_2_selex_f_qcs_mean_ci <- paste0(f(sens_3_2_selex_f_qcs[2], 1),
                                       " (",
                                       f(sens_3_2_selex_f_qcs[1], 1),
                                       "--",
                                       f(sens_3_2_selex_f_qcs[3], 1),
                                       ")")

# This is a hard coded value in the abstract (percentage of posteriors below the 0.2B0 LRP)
prob_below_02_sbo_2022 <- sum(unlist(base_model$mcmccalcs$depl[, ncol(base_model$mcmccalcs$depl)]) < 0.2) / 
  nrow(base_model$mcmccalcs$depl) * 100

prob_above_02_sbo_2022 <- sum(unlist(base_model$mcmccalcs$depl[, ncol(base_model$mcmccalcs$depl)]) > 0.2) / 
  nrow(base_model$mcmccalcs$depl) * 100

prob_above_04_sbo_2022 <- sum(unlist(base_model$mcmccalcs$depl[, ncol(base_model$mcmccalcs$depl)]) > 0.4) / 
  nrow(base_model$mcmccalcs$depl) * 100

split_sex_model_sel <- models$bridge_grps[[2]][[4]]$mcmccalcs$selest_quants |> filter(gear == 6, sex == 2)
split_sex_model_sel_ahat <- paste0(f(split_sex_model_sel$a_hat[2]), " (", 
                                   f(split_sex_model_sel$a_hat[1]), "--",
                                   f(split_sex_model_sel$a_hat[3]), ")")
split_sex_model_sel_ghat <- paste0(f(split_sex_model_sel$g_hat[2]), " (", 
                                   f(split_sex_model_sel$g_hat[1]), "--",
                                   f(split_sex_model_sel$g_hat[3]), ")")

age_50_sel <- base_model$mcmccalcs$params_quants |> 
  as_tibble(rownames = "quant") |>
  filter(quant == "50%") |>
  select(contains("sel")) |> 
  select(contains("age50"))
mean_female_age_50_sel <- age_50_sel |> 
  select(contains("female")) |> 
  unlist() |> 
  mean()
mean_male_age_50_sel <- age_50_sel |> 
  select(contains("male")) |> 
  unlist() |> 
  mean()

vuln_ratio_yr <- "2021"
base_vuln_bio <-
  base_model$mcmccalcs$vbt_quants[[1]][, vuln_ratio_yr][2] + base_model$mcmccalcs$vbt_quants[[2]][, vuln_ratio_yr][2]

base_model_bio <- base_model$mcmccalcs$sbt_quants[, vuln_ratio_yr][2]
base_model_vuln_ratio <- base_vuln_bio / base_model_bio

sel_eq_mat_vuln_bio <-
  models$sens_grps[[4]][[2]]$mcmccalcs$vbt_quants[[1]][, vuln_ratio_yr][2] + models$sens_grps[[4]][[2]]$mcmccalcs$vbt_quants[[2]][, vuln_ratio_yr][2]

sel_eq_mat_bio <- models$sens_grps[[4]][[2]]$mcmccalcs$sbt_quants[, vuln_ratio_yr][2]

sel_eq_mat_vuln_ratio <- sel_eq_mat_vuln_bio / sel_eq_mat_bio

```

```{r removal-rate-calcs}

# This was returned from a call to find_f_b40(base_model)
# and represent the F and U values that it would take to drive the spawning biomass
# to 0.4B0 from 2022 to 2023. The catch at those rates is also included.
f_bo_40 <- list(f = c(f_fleet1 = 0.06636655,
                      f_fleet2 = 0.04168355),
                u = c(u_fleet1 = 0.06421221,
                      u_fleet2 = 0.04082674),
                catch = 4.40625)
# Difference between 0.4B0 and biomass after 50 years (kt)
diff_bio_b40 <- f(0.0064 * 1000, 1)

# extract_fleet_f <- function(d, fleet = 1){
# 
#   fleet_str <- ifelse(fleet == 1, "flt1$", "flt2$")
#   
#   d <- d |> 
#     select(catch, which(grepl(fleet_str, names(d))))
#   
#   names(d) <- gsub("_flt[1|2]$", "", names(d))
# 
#   d |> 
#     mutate(fleet = !!fleet) |> 
#     select(catch, fleet, everything())
# }
# 
# extract_sex_f <- function(d, sex = "f"){
# 
#   sex_str <- ifelse(sex == "f", "sex1$", "sex2$")
#   d <- d |> 
#     select(catch, which(grepl(sex_str, names(d))))
# 
#   names(d) <- gsub("_sex[1|2]$", "", names(d))
# 
#   out_lst <- list()
#   out_lst$fleet1 <- extract_fleet_f(d, 1) |> 
#     mutate(sex = !!sex) |> 
#     select(catch, fleet, sex, everything())
# 
#   out_lst$fleet2 <- extract_fleet_f(d, 2) |> 
#     mutate(sex = !!sex) |> 
#     select(catch, fleet, sex, everything())
#   
#   out_lst |> 
#     bind_rows()
# }

# j <- imap(basep$mcmccalcs$proj_quants, ~{
#   tib <- as_tibble(.x, rownames = "quants") |> mutate(catch = .y)
#   nms <- names(tib)
#   wch <- grep("^(F|U)20[0-9]{2}_flt[1|2]_sex[1|2]$", nms, value = T)
#   tib |>
#     select(c(catch, quants, all_of(wch))) |>
#     filter(quants == "50%") |> 
#     mutate(catch = as.numeric(catch)) |> 
#     select(-quants)
# }) |> 
#   map_df(~{.x})

# Calc probability that F < F_B40, first extract projection posteriors
# tib <- basep$mcmccalcs$proj |> 
#   bind_rows() |> 
#   rename(catch = TAC)
# nms <- names(tib)
# wch <- grep("^(F|U)20[0-9]{2}_flt[1|2]_sex[1|2]$", nms, value = T)
# j <- tib |>
#     select(c(catch, all_of(wch)))
# 
# f_lst <- list()
# f_lst$female <- extract_sex_f(j, "f")
# f_lst$male <- extract_sex_f(j, "m")
# f_df <- bind_rows(f_lst)
# 
# # Only keep 2022 F's and U's
# wch <- grep("F2022", names(f_df), value = T)
# f_ <- f_df |> 
#   select(catch, fleet, wch) |> 
#   group_by(catch, fleet) |> 
#   summarize(sum(F2022 < f_bo_40$f[1]))
# 
# wch <- grep("U2022", names(f_df), value = T)
# u_ <- f_df |> 
#   select(catch, fleet, sex, wch)

# k <- f_df |>
#   mutate(f_bo_40 = ifelse(fleet == 1, !!f_bo_40$f[["f_fleet1"]], !!f_bo_40$f[["f_fleet2"]]),
#          u_bo_40 = ifelse(fleet == 1, !!f_bo_40$u[["u_fleet1"]], !!f_bo_40$u[["u_fleet2"]])) |> 
#   mutate(F2022_F40 = F2022 / f_bo_40,
#          U2022_U40 = U2022 / u_bo_40) |> 
#   select(-c(F2022, U2022, f_bo_40, u_bo_40))
  

figures_dir <- here(rmarkdown::metadata$output$`csasdown::sr_pdf`$figures_dir)
accessible_pf <- rmarkdown::metadata$output$`csasdown::sr_pdf`$accessible_pdf

```

<!-- For highlighting table cells for readability. See the decision table code for example.  -->
\definecolor{faint-gray}{gray}{0.9}

