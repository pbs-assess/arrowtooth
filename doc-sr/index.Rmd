---
title: "Stock Status update for Arrowtooth Flounder (*Atheresthes stomias*) for the West Coast of British Columbia in 2024"
french_title: "Mise à jour de l'état des stocks de flétan noir (*Atheresthes stomias*) pour la côte ouest de la Colombie-Britannique en 2024"
title_short: "Arrowtooth draft SR -- Do not cite or circulate" #"Stock Status Update for Arrowtooth Flounder in 2023"
citation_english: "DFO, Stock Status update for Arrowtooth Flounder (*Atheresthes stomias*) for the West Coast of British Columbia in 2024. DFO Can. Sci. Advis. Sec. Res. Doc. 2024/nnn."
citation_french: "MPO, Mise à jour de l'état des stocks de la plie à dents de flèche (*Atheresthes stomias*) pour la côte ouest de la Colombie-Britannique en 2024. DFO Secr. can. de consult. sci. du MPO. Doc. de rech 2024/nnn."
model_path: "/srv/arrowtooth/2022"
nongit_path: "/srv/arrowtooth/arrowtooth-nongit"
base_model_path: "/srv/arrowtooth/2024/01-base-models/01-base-model"
year: 2024
month: "September"
french_month: "Septembre"
report_number: 999 # This *must* be numeric! E.g., not `nnn`.
approver: |
  Andrew Thompson\
  Regional Director
approval_year: 2024
approval_month: "September"
approval_day: 11
work_done_year: 2024
work_done_month: "September"
branch: Science Branch # French: Direction des sciences
region: Pacific Region
french_region: Région du Pacifique
isbn: "ISBN-Here"
cat_no: "Cat-Number-Here"
# `show_continued_text` is a logical which, if `true`, places
# "Continued on the next page..." and "...Continued from the previous page" or
# the french equivalents (if `french` = `true`) on all long tables created
# with `csas_table()` that cross page boundaries. If `false`, these will
# both be absent from all tables. If it is missing or any other value than
# `false`, it will be assumed to be `true`
show_continued_text: false
output:
 csasdown::sr_pdf:
   # `lualatex` is required for `accessibile_pdf` to work
   latex_engine: lualatex
   # If `true`, alternative figure text and figure tags are added for
   # PDF web accessibility compliance
   accessible_pdf: true
   # The name of the directory containing pre-made figures such as png files
   # that will be included using `include_graphics()`
   figures_dir: figure
   # This value will be the return value for `fr()` in your code
   french: false
   prepub: false
   # copy_sty is a toggle to copy the style file from the csasdown package every time you compile
   # the document. If false, any changes you have made to the style file in your project
   # will remain between compilations. If true, your changes will be lost when you compile
   copy_sty: true
   # line_nums is a toggle to show line numbers on the left side of the page. 
   line_nums: false
   # line_nums_mod represents showing every Nth line if line_nums is true
   line_nums_mod: 1
   # draft_watermark is a toggle to show/not show a DRAFT watermark across every page
   draft_watermark: false
   # highlight is the theme to use for code output. Must be one of the list given by:
   # pandoc --list-highlight-styles
   # which are:
   # pygments, tango, espresso, zenburn, kate, monochrome, breezedark, haddock
   # or the name of a custom *.latex file which is most easily made by copying one from 
   # the csasdown library 'themes' directory, this directory on your machine:
   # file.path(.libPaths(), "csasdown", "themes")
   # to your working directory (the one containing index.Rmd)
   # To change the foreground text color, change the RGB value in the line containing
   # 'DefineVerbatimEnvironment'
   # To change background color, change the RGB values in the line containing 'shadecolor'
   highlight: tango
type:
  sr
# ------------
# End of options to set
knit: (function(input, ...) csasdown::render())
site: bookdown::bookdown_site
link-citations: true
bibliography: ../doc/bib/refs.bib
lot: true
lof: true
# Any extra LaTeX code for the header:
# header-includes:
# - \usepackage{tikz}
---

```{r setup, echo = FALSE, cache = FALSE, message = FALSE, results = "hide", warning = FALSE}

curr_dir <- basename(getwd())
curr_dir_up1 <- basename(dirname(getwd()))
if(curr_dir != "doc" && curr_dir_up1 != "arrowtooth"){
  stop("You must be in the 'arrowtooth/doc' directory to source this file.\n",
       "The current directory is: ", getwd())
}

library(knitr)
if (is_latex_output()) {
  knitr_figs_dir <- "knitr-figs-pdf/"
  knitr_cache_dir <- "knitr-cache-pdf/"
} else {
  knitr_figs_dir <- "knitr-figs-docx/"
  knitr_cache_dir <- "knitr-cache-docx/"
}
fig_asp <- 0.618
fig_width <- 8
fig_out_width <- "5.5in"

user <- Sys.info()[["user"]]
opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  results = 'hide',
  comment = "#>",
  fig.path = knitr_figs_dir,
  cache.path = knitr_cache_dir,
  fig.asp = fig_asp,
  fig.width = fig_width,
  out.width = fig_out_width,
  echo = FALSE,
  # autodep = isTRUE(user %in% "seananderson"),
  # cache = isTRUE(user %in% "seananderson"),
  cache.comments = FALSE,
  # These two lines needed for the maps with geom_sf() to be rendered
  # correctly
  dev = "ragg_png",
  fig.ext = "png",
  dpi = 180,
  fig.align = "center",
  fig.pos = "H")
options(
  # Prevent xtable from adding a timestamp comment to the table code
  # it produces
  xtable.comment = FALSE,
  # Don't allow kableExtra to load packages, we add them manually in
  # csasdown
  kableExtra.latex.load_packages = FALSE,
  # Stop chunk output (echo) running into the margins
  width = 80,
  # Don't use scientific notation (stops tables from showing 1.2e3, etc.)
  scipen = 999)
# Fixes weird bug where knitr::include_graphics() thinks the non-git folder
# is relative
options(knitr.graphics.rel_path = FALSE)
```

```{r library-setup, cache = FALSE, fig.keep='none'}
# Libraries in alphabetical order

library(devtools)
library(dplyr)
if(as.logical(length(grep("grandin", user)))){
  load_all("~/github/kableExtra")
  load_all("~/github/pbs-assess/gfiscamutils")
  load_all("~/github/pbs-assess/gfplot")
  load_all("~/github/pbs-assess/csasdown")
  load_all("~/github/pbs-assess/rosettafish")
}else if(user == "seananderson"){
  library(gfiscamutils)
  load_all("~/src/gfiscamutils/")
  load_all("~/src/csasdown/")
  # load_all("~/src/gfplot/")
  library(gfplot)
}else{
  library(gfiscamutils)
  library(gfplot)
  library(csasdown)
}
library(gfutilities)
library(ggplot2)
library(gridExtra)
library(here)
#library(kableExtra)
library(purrr)
library(rosettafish)
library(tidylog, warn.conflicts = FALSE)
load_all(".")

yaml_tags <- rmarkdown::metadata
meta <- yaml_tags$output
curr_year <- yaml_tags$year
build_rds <- FALSE
if(!is.null(meta)){
  build_rds <- meta$`csasdown::resdoc_pdf`$build_rds
}
```

```{r include = FALSE}
# Don't load the models if they already exist
if(!exists("models") || !exists("drs"))
  source("../doc/load-models.R", local = knitr::knit_global())
```

```{r data-setup, cache.lazy = FALSE}
# This chunk requires the chunk above that loads load-models.R has been run

bc <- tr("British Columbia")
sp <- tr("Arrowtooth Flounder")
iscam <- "ISCAM"

month_fishing_starts <- 2
day_fishing_starts <- 21

data_dir <- file.path(drs$nongit_dir, "data")
data_output_dir <- file.path(drs$nongit_dir, "data-output")

if(!dir.exists(data_dir)){
  stop("Data directory does not exist: ", data_dir, call. = FALSE)
}
iphc_file <- file.path(data_dir, "iphc-survey-index.rds")
if(!file.exists(iphc_file)){
  stop("IPHC file does not exist: ", iphc_file, call. = FALSE)
}
discard_cpue_file <- file.path(data_dir,
"cpue-predictions-arrowtooth-flounder-modern-3CD5ABCDE-2024.csv")
# Used in 2022 assessment:
#"cpue-predictions-arrowtooth-flounder-modern-3CD5ABCDE-discard-july-26-feb-fishing-year.csv")
#
if(!file.exists(discard_cpue_file)){
  stop("Discard CPUE file does not exist: ", discard_cpue_file, call. = FALSE)
}
stitched_syn_file <- file.path(data_dir, "stitched-syn-index.rds")
if(!file.exists(stitched_syn_file)){
  stop("Stitched Synoptics file does not exist: ", stitched_syn_file, call. = FALSE)
}

iphc <- readRDS(iphc_file)$series_ABCD_full$ser_longest
discard_cpue <- read_csv(discard_cpue_file)
stitched_syn <- readRDS(stitched_syn_file)

dat <- readRDS(file.path(drs$nongit_dir, "data",
                         #"arrowtooth-flounder-aug11-2022.rds"))
                         "arrowtooth-flounder-june25-2024.rds"))

# Remove 2014 WCHG index point
wchg_2014_row <- 
  dat$survey_index$survey_abbrev == "SYN WCHG" & dat$survey_index$year == 2014
if(any(wchg_2014_row)){
  dat$survey_index <- dat$survey_index[-which(wchg_2014_row), ]
}

# These must be removed for call to add_extra_indices()
survey_index <- dat$survey_index |> 
  select(-species_common_name, -species_science_name)
survey_index <- add_extra_indices(survey_index, 
                                  iphc = iphc,
                                  discard_cpue = discard_cpue,
                                  stitched_syn = stitched_syn)
# Survey index for geostat
geo_files <- dir(file.path(drs$nongit_dir, "survey-geostat"),
                 full.names = TRUE,
                 pattern = "^i-arrowtooth.*\\.rds$")
ind_geo <- purrr::map_dfr(geo_files, readRDS)
survey_index_geo <- ind_geo |>  filter(model == "no-depth") |> as_tibble()
survey_index_geo <- survey_index_geo |>
  rename(biomass = est,
         lowerci = lwr,
         upperci = upr,
         re = se,
         survey_abbrev = surveys) |>
  mutate(num_sets = NA,
         num_pos_sets = NA,
         survey_series_id = NA,
         survey_series_desc = "") |>
  select(-c(log_est, species, survey, ssids, ssid_string,
            family, anisotropy, spatiotemporal, share_range,
            model, max_grad)) |>
  select(year, biomass, lowerci, upperci,
         re, num_sets, num_pos_sets,
         survey_series_id, survey_abbrev, survey_series_desc) |>
  add_extra_indices(discard_cpue = discard_cpue)
# Add HS MSA survey so plot will work
hs_multi <- survey_index |> filter(survey_abbrev == "OTHER HS MSA")
survey_index_geo <- survey_index_geo |>
  bind_rows(hs_multi)

# Areas 3CD and 5ABCDE only 
major_areas <- c("03","04", "05", "06", "07", "08", "09")
tidy_areas <- c("3[CD]+", "5[ABCDE]+")
survey_sets <- dat$survey_sets |> 
  filter(major_stat_area_code %in% major_areas)
survey_samples <- dat$survey_samples |> 
  filter(major_stat_area_code %in% major_areas)
survey_samples_syn <- survey_samples |> 
  filter(survey_abbrev %in% c("SYN QCS",
                              "SYN HS",
                              "SYN WCVI",
                              "SYN WCHG"))
commercial_samples <- readRDS(file.path(drs$nongit_dir,
                                        "data",
                                        "arrowtooth-commercial-samples-sept-12-2024.rds")) |> 
  filter(major_stat_area_code %in% major_areas)
comm_ft <- extract_fleet_samples(commercial_samples)
comm_ss <- extract_fleet_samples(commercial_samples, include = FALSE)

# Aggregated commercial catch
month_start <- 2
day_start <- 21
dat$catch <- dat$catch |> filter(year < curr_year)
catch <- tidy_catch(dat$catch,
                    areas = tidy_areas,
                    month_fishing_starts = month_start,
                    day_fishing_starts = day_start)
# Catch by fleet
catch_ft <- extract_fleet_catch(dat$catch) |> 
  tidy_catch(areas = tidy_areas,
             month_fishing_starts = month_start,
             day_fishing_starts = day_start)

catch_ss <- extract_fleet_catch(dat$catch, include = FALSE) |> 
  tidy_catch(areas = tidy_areas,
             month_fishing_starts = month_start,
             day_fishing_starts = day_start)

cpue_spatial <- dat$cpue_spatial
cpue_spatial_ll <- dat$cpue_spatial_ll
age_precision <- dat$age_precision

theme_set(gfiscam_theme())

if(!exists("curr_gear_language")){
  curr_gear_language <- "en"
}

models <- set_gear_language(models, curr_gear_language)

base_model <- models$base_grps[[1]][[1]]
curr_gear_language <- ifelse(fr(), "fr", "en")

base_all_gears <- gear_lu_table(base_model, "all")
base_age_gears <- gear_lu_table(base_model, "age")
base_index_gears <- gear_lu_table(base_model, "index")
base_fleet_gears <- gear_lu_table(base_model, "fleet")

mcmc_chain_length <- 10000000
mcmc_num_samples <- 2000
mcmc_sample_freq <- mcmc_chain_length / mcmc_num_samples
mcmc_burn_in <- 1000
mcmc_actual_samples <- mcmc_num_samples - mcmc_burn_in

qcs <- tr("Queen Charlotte Sound Synoptic Survey")
hsmas <- tr("Hecate Strait Multispecies Assemblage Survey")
hss <- tr("Hecate Strait Synoptic Survey")
wcvis <- tr("West Coast Vancouver Island Synoptic Survey")
wchgs <- tr("West Coast Haida Gwaii Synoptic Survey")
dcpue <- tr("Discard CPUE Index")

la <- ifelse(fr(), "Évaluation de 2015", "2015 assessment")

# Number of parameters estimated (from PAR file)
num_params <- get_num_params_est(base_model)

# Catch table
ct <- as_tibble(base_model$dat$catch)
ct_start_yr <- min(ct$year)
ct_end_yr <- max(ct$year)

# Reference points (table values)
ref_pts <- as_tibble(base_model$mcmccalcs$params_quants)

# Projected biomass
end_yr <- base_model$dat$end.yr
assess_yr <- end_yr + 1
proj_yr <- assess_yr + 1
sbt_quants <- as_tibble(base_model$mcmccalcs$sbt_quants)
proj_bio <- sbt_quants[, as.character(assess_yr)] |> pull()

# Fishing mortality
f_max_by_gear <- map_dbl(base_model$mcmccalcs$ft_quants, ~{
  max(.x[2,])
})
f_max <- max(f_max_by_gear)
which_f_max <- which(f_max == f_max_by_gear)
which_f_max_gear <- base_model$dat$fleet_gear_names[which_f_max]
which_f_max_yr <- names(which(base_model$mcmccalcs$ft_quants[[which_f_max]][2, ] == f_max))

f_ci <- base_model$mcmccalcs$ft_quants[[which_f_max]][, base_model$mcmccalcs$ft_quants[[which_f_max]][2, ] == f_max]

# Relative spawning biomass
depl_end <- as_tibble(base_model$mcmccalcs$depl_quants) |>
  select(!!sym(as.character(assess_yr))) |> 
  pull()

# Columns of table_dec are:
# Catch, P(B2023<0.2B0), P(B2024<0.2B0), P(B2025<0.2B0), P(B2023<0.2B0),
# P(B2023<0.4B0), P(B2024<0.4B0), P(B2025<0.4B0), P(B2023<0.4B0),
# P(B2023<B2022), P(B2024<B2023), P(B2025<B2024)
table_dec <- table_decisions(base_model, ret_df = TRUE, digits = 3)
catch_col <- sym(grep(tr("Catch"), names(table_dec), value = TRUE))

# Decision table column indices for P(B_2023<B_2022)
i_2025 <- grep("\\{2025\\} < B", names(table_dec))
i_2026 <- grep("\\{2026\\} < B", names(table_dec))
i_2027 <- grep("\\{2027\\} < B", names(table_dec))

prob_2025_2024_catch_10 <- table_dec |>
  filter(!!catch_col == 10) |> pull(i_2025)
prob_2026_2025_catch_10 <- table_dec |>
  filter(!!catch_col == 10) |> pull(i_2026)
prob_2027_2026_catch_10 <- table_dec |>
  filter(!!catch_col == 10) |> pull(i_2027)

# Decision table column indices for P(B_2023<0.4B0) and P(B_2023<0.4B0)
i_2024_4bo <- grep("\\{2025\\} < 0.4B", names(table_dec))
i_2024_2bo <- grep("\\{2025\\} < 0.2B", names(table_dec))

```

```{r biological-params}

find_length_outliers <- function(xx) {
  yy <- stats::pnorm(xx,
    mean = mean(xx, na.rm = TRUE),
    sd = stats::sd(xx, na.rm = TRUE), log.p = TRUE
  )
  zz <- stats::qnorm(yy, log.p = TRUE)
  out <- zz[zz > 4 & !is.na(zz)]
  if (length(out) > 1L) {
    return(xx[which(zz > 4)])
  } else {
    return(numeric(0))
  }
}

length_samples_survey <- filter(
  dat$survey_samples,
  !length %in% find_length_outliers(dat$survey_samples$length)
)

length_samples_ft <- filter(
  comm_ft,
  !length %in% find_length_outliers(comm_ft$length)
)

length_samples_ss <- filter(
  comm_ss,
  !length %in% find_length_outliers(comm_ss$length)
)

all_length_samples <- bind_rows(length_samples_survey,
                                length_samples_ft,
                                length_samples_ss)

all_age_samples <- bind_rows(dat$survey_samples, comm_ft, comm_ss) |>
  filter(!is.na(age) & age < 40)

# Use function from this package as it is (very) slightly different than what 
# fit_mat_ogive() returns, and is what is input into the model
mat_fit <- export_mat_lw_age(dat$survey_samples, write_file = FALSE)
# TODO: what random effects wanted? If year, than params are saved as
# mat_fit$mat_perc$mean$f.mean.p0.5 and mat_fit$mat_perc$mean$m.mean.p0.5
# instead of mat_fit$mat_perc$f.p0.5 and mat_fit$mat_perc$m.p0.5. I assume
# fig:fig-mat should also be made to match 

# Natural mortality values in the control file
param_ctl_table <- models$bridge_grps[[3]][[2]]$ctl$params |>
  as_tibble(rownames = "param")
male_m_ctl <- exp(param_ctl_table |> filter(param == "log_m_male") |>
                    pull(ival))
female_m_ctl <- exp(param_ctl_table |> filter(param == "log_m_female") |>
                      pull(ival))
```

```{r proportion-female}

if(!exists("data_dir")){
  stop("`data_dir` does not exist. If running from command line, ",
       "source('index.Rmd') to set up all project variables", call. = FALSE)
}

prop_female_fn <- file.path(data_dir, "prop_female_output.rds")
if(file.exists(prop_female_fn)){
  prop_female_lst <- readRDS(prop_female_fn)
}else{
  comm_prop <- props_comm(dat$commercial_samples)
  surv_prop <- props_surv(surv_series = c(1, 3, 4, 16),
                          surv_series_names = c("qcsss",
                                                "hsss",
                                                "wcviss",
                                                "wchgss"),
                          surv_samples = dat$survey_samples,
                          surv_sets = dat$survey_sets)
  prop_female_lst <- list(comm_prop, surv_prop)
  saveRDS(prop_female_lst, prop_female_fn)
}

prop_female_means <- table_prop_female(prop_female_lst,
                                       ret_means = TRUE)

total_prop_female <- f(mean(prop_female_means), 2)
```

```{r model-param-value-calcs}

base_sbo <- get_parvals(base_model, "sbo")
base_bo <- get_parvals(base_model, "bo")
base_sbt <- get_parvals(base_model, "sbt")
base_depl <- get_parvals(base_model, "depl", digits = 2)
base_m_male <- get_parvals(base_model, "m_male", digits = 2)
base_m_female <- get_parvals(base_model, "m_female", digits = 2)
base_h <- get_parvals(base_model, "h", digits = 2)

bvals <- get_group_parvals(models$bridge_grps)
svals <- get_group_parvals(models$sens_grps)

# Extract parameter values from the table found in the control file
#
# @param model The iSCAM model
# @param param The parameter name (row)
# @param value The value (column)
# @param digits The number of decimal points to return
#
# @return The value or row
# @export
get_ctl_params <- function(model, param = NULL, value = NULL, ...){
  inp_params <- as_tibble(rownames_to_column(as.data.frame(model$ctl$params),
                                             var = "param"))

  if(!is.null(param)){
    inp_params <- filter(inp_params, param == !!param)
  }
  if(!is.null(value)){
    return(pull(inp_params, value))
  }
  inp_params
}

get_param_est <- function(model, param = NULL, est_digits = 2, ...){
  if(is.null(param)){
    stop("Must provode `param` name", call. = FALSE)
  }
  
  if(param == "log_m_female"){
    param ="m_sex1"
  }
  if(param == "log_m_male"){
    param ="m_sex2"
  }
  raw <- as_tibble(model$mcmccalcs$params_quants)[[param]]
  paste0(f(raw[2], est_digits),
         " (",
         f(raw[1], est_digits),
         "--",
         f(raw[3], est_digits),
         ")")
}

get_param_vals <- function(model, param, est = TRUE, ...){
  out <- NULL
  out$init <- get_ctl_params(model, param, "ival", ...)
  out$p1 <- get_ctl_params(model, param, "p1", ...)
  out$p2 <- get_ctl_params(model, param, "p2", ...)
  if(est){
    out$est <- get_param_est(model, param, ...)
  }
  out
}

base_vartheta <- get_param_vals(base_model, "vartheta", est = FALSE, digits = 5)
base_rho <- get_param_vals(base_model, "rho", est = FALSE, digits = 5)
base_sig_tau <- calc_sig_tau(get_param_vals(base_model, "rho", est = FALSE)$init,
                             get_param_vals(base_model, "vartheta", est = FALSE)$init)
base_sig <- f(base_sig_tau[1], 1)
base_tau <- f(base_sig_tau[2], 1)

base_h <- get_param_vals(base_model, "h")
base_h_prior1 <- calc_beta_mean_cv(base_h$p1, base_h$p2)[1]
base_h_prior2 <- calc_beta_mean_cv(base_h$p1, base_h$p2)[2]
base_h_prior_params <- paste(f(base_h_prior1, 2),
                             ",",
                             f(base_h_prior2, 2))
base_m_female <- get_param_vals(base_model, "log_m_female")
base_m_male <- get_param_vals(base_model, "log_m_male")

sens_1_2_vartheta <- get_param_vals(models$sens_grps[[1]][[2]], "vartheta")
sens_1_2_rho <- get_param_vals(models$sens_grps[[1]][[2]], "rho", est = FALSE)
sens_1_2_sig_tau <- calc_sig_tau(get_param_vals(models$sens_grps[[1]][[2]],
                                                           "rho", est = FALSE)$init,
                                 get_param_vals(models$sens_grps[[1]][[2]],
                                                           "vartheta")$init)
sens_1_2_sig <- f(sens_1_2_sig_tau[1], 3)
sens_1_2_tau <- f(sens_1_2_sig_tau[2], 1)
sens_1_2_h <- get_param_vals(models$sens_grps[[1]][[2]], "h")
sens_1_2_sbo <- get_param_vals(models$sens_grps[[1]][[2]], "sbo", est_digits = 0)

sens_1_3_vartheta <- get_param_vals(models$sens_grps[[1]][[3]], "vartheta")
sens_1_3_rho <- get_param_vals(models$sens_grps[[1]][[3]], "rho", est = FALSE)
sens_1_3_sig_tau <- calc_sig_tau(get_param_vals(models$sens_grps[[1]][[3]],
                                                           "rho", est = FALSE)$init,
                                 get_param_vals(models$sens_grps[[1]][[3]],
                                                           "vartheta")$init)
sens_1_3_sig <- f(sens_1_3_sig_tau[1], 1)
sens_1_3_tau <- f(sens_1_3_sig_tau[2], 1)
sens_1_3_sbo <- get_param_vals(models$sens_grps[[1]][[3]], "sbo", est_digits = 0)


sens_1_4_vartheta <- get_param_vals(models$sens_grps[[1]][[4]], "vartheta")
sens_1_4_rho <- get_param_vals(models$sens_grps[[1]][[4]], "rho", est = FALSE)
sens_1_4_sig_tau <- calc_sig_tau(get_param_vals(models$sens_grps[[1]][[4]],
                                                           "rho", est = FALSE)$init,
                                 get_param_vals(models$sens_grps[[1]][[4]],
                                                           "vartheta")$init)
sens_1_4_sbo <- get_param_vals(models$sens_grps[[1]][[4]], "sbo")

sens_1_4_sig <- f(sens_1_4_sig_tau[1], 1)
sens_1_4_tau <- f(sens_1_4_sig_tau[2], 1)

sens_1_5_h <- get_param_vals(models$sens_grps[[1]][[5]], "h")
sens_1_5_h_prior1 <- calc_beta_mean_cv(sens_1_5_h$p1, sens_1_5_h$p2)[1]
sens_1_5_h_prior2 <- calc_beta_mean_cv(sens_1_5_h$p1, sens_1_5_h$p2)[2]
sens_1_5_h_prior_params <- paste(f(sens_1_5_h_prior1, 2),
                             ",",
                             f(sens_1_5_h_prior2, 2))

sens_2_2_m_female <- get_param_vals(models$sens_grps[[2]][[2]], "log_m_female")
sens_2_3_m_female <- get_param_vals(models$sens_grps[[2]][[3]], "log_m_female")
sens_2_4_m_male <- get_param_vals(models$sens_grps[[2]][[4]], "log_m_male")
sens_2_5_m_male <- get_param_vals(models$sens_grps[[2]][[5]], "log_m_male")

# qk priors and estimates
base_qk_inp_params <- base_model$ctl$surv.q
base_qk_mean <- exp(base_qk_inp_params[rownames(base_qk_inp_params) == "priormeanlog"])[1]
base_qk_sd <- base_qk_inp_params[rownames(base_qk_inp_params) == "priorsd"][1]

sens_qk_inp_params <- models$sens_grps[[3]][[2]]$ctl$surv.q
sens_qk_mean <- exp(sens_qk_inp_params[rownames(sens_qk_inp_params) == "priormeanlog"])[1]
sens_qk_sd <- sens_qk_inp_params[rownames(sens_qk_inp_params) == "priorsd"][1]

sens_qkp_inp_params <- models$sens_grps[[3]][[3]]$ctl$surv.q
sens_qkp_mean <- exp(sens_qkp_inp_params[rownames(sens_qkp_inp_params) == "priormeanlog"])[1]
sens_qkp_sd <- sens_qkp_inp_params[rownames(sens_qkp_inp_params) == "priorsd"][1]

sens_3_2_selex_f_qcs <- filter(models$sens_grps[[3]][[2]]$mcmccalcs$selest_quants, gear == "QCS Synoptic", sex == 2)$a_hat
sens_3_2_selex_f_qcs_mean_ci <- paste0(f(sens_3_2_selex_f_qcs[2], 1),
                                       " (",
                                       f(sens_3_2_selex_f_qcs[1], 1),
                                       "--",
                                       f(sens_3_2_selex_f_qcs[3], 1),
                                       ")")

# This is a hard coded value in the abstract (percentage of posteriors below the 0.2B0 LRP)
prob_below_02_sbo_nextyr <-
  sum(unlist(base_model$mcmccalcs$depl[, ncol(base_model$mcmccalcs$depl)]) < 0.2) /
  nrow(base_model$mcmccalcs$depl) * 100

prob_above_02_sbo_nextyr <-
  sum(unlist(base_model$mcmccalcs$depl[, ncol(base_model$mcmccalcs$depl)]) > 0.2) /
  nrow(base_model$mcmccalcs$depl) * 100

prob_above_04_sbo_nextyr <-
  sum(unlist(base_model$mcmccalcs$depl[, ncol(base_model$mcmccalcs$depl)]) > 0.4) / 
  nrow(base_model$mcmccalcs$depl) * 100

split_sex_model_sel <-
  models$bridge_grps[[2]][[4]]$mcmccalcs$selest_quants |>
  filter(gear == 6, sex == 2)
split_sex_model_sel_ahat <- paste0(f(split_sex_model_sel$a_hat[2]), " (", 
                                   f(split_sex_model_sel$a_hat[1]), "--",
                                   f(split_sex_model_sel$a_hat[3]), ")")
split_sex_model_sel_ghat <- paste0(f(split_sex_model_sel$g_hat[2]), " (", 
                                   f(split_sex_model_sel$g_hat[1]), "--",
                                   f(split_sex_model_sel$g_hat[3]), ")")

age_50_sel <- base_model$mcmccalcs$params_quants |> 
  as_tibble(rownames = "quant") |>
  filter(quant == "50%") |>
  select(contains("sel")) |> 
  select(contains("age50"))
mean_female_age_50_sel <- age_50_sel |> 
  select(contains("female")) |> 
  unlist() |> 
  mean()
mean_male_age_50_sel <- age_50_sel |> 
  select(contains("male")) |> 
  unlist() |> 
  mean()

vuln_ratio_yr <- as.character(end_yr)
base_vuln_bio <-
  base_model$mcmccalcs$vbt_quants[[1]][, vuln_ratio_yr][2] +
  base_model$mcmccalcs$vbt_quants[[2]][, vuln_ratio_yr][2]

base_model_bio <- base_model$mcmccalcs$sbt_quants[, vuln_ratio_yr][2]
base_model_vuln_ratio <- base_vuln_bio / base_model_bio

if(vuln_ratio_yr %in% colnames(models$sens_grps[[4]][[2]]$mcmccalcs$vbt_quants[[1]])){
  sel_eq_mat_vuln_bio <-
    models$sens_grps[[4]][[2]]$mcmccalcs$vbt_quants[[1]][, vuln_ratio_yr][2] +
    models$sens_grps[[4]][[2]]$mcmccalcs$vbt_quants[[2]][, vuln_ratio_yr][2]
  
  sel_eq_mat_bio <- models$sens_grps[[4]][[2]]$mcmccalcs$sbt_quants[, vuln_ratio_yr][2]
  sel_eq_mat_vuln_ratio <- sel_eq_mat_vuln_bio / sel_eq_mat_bio
}

# Current assessment text
ct_df <- table_catch_fleet(list(catch_ft, catch_ss),
                           base_model$dat$fleet_gear_names,
                           show_total_col = TRUE,
                           ret_df = TRUE)
# ct_df column 1 is Year, and last column is the total catch for all gears
# Each fleet has three columns, landings, discards, and total of the two
names(ct_df) <- c("Year", seq_len(ncol(ct_df))[-1])
ct_ft_2021 <- ct_df |> 
  filter(Year == 2021) |> 
  pull(`4`)
ct_ft_2022 <- ct_df |> 
  filter(Year == 2022) |> 
  pull(`4`)
ct_ft_2023 <- ct_df |> 
  filter(Year == 2023) |> 
  pull(`4`)
perc_inc_ft_2021_2022 <- f(ct_ft_2022 / ct_ft_2021 * 100 - 100, 1)
perc_dec_ft_2022_2023 <- f(100 - ct_ft_2023 / ct_ft_2022 * 100, 1)
ct_ss_2021 <- ct_df |> 
  filter(Year == 2021) |> 
  pull(`7`)
ct_ss_2022 <- ct_df |> 
  filter(Year == 2022) |> 
  pull(`7`)
ct_ss_2023 <- ct_df |> 
  filter(Year == 2023) |> 
  pull(`7`)
perc_dec_ss_2021_2022 <- f(100 - ct_ss_2022 / ct_ss_2021 * 100, 1)
perc_dec_ss_2022_2023 <- f(100 - ct_ss_2023 / ct_ss_2022 * 100, 1)

# create and filter an MCMC diagnostic data frame with one value column
# (second column parameter name) for rstan Rhat and ESS values
create_mcmc_diag <- function(df,
                             fun = rstan::Rhat,
                             digits = 3,
                             value_nm = "Rhat"){
  
  d <- df |>
  map_dbl(~{fun(.x)}) |>
  enframe() |> 
  filter(!is.na(value)) |> 
  filter(!grepl("^sel", name)) |> 
  filter(!grepl("msy", name)) |> 
  filter(!grepl("ssb|beta", name)) |> 
  filter(!grepl("^bo$", name)) |> 
  filter(!grepl("^so$", name)) |> 
  mutate(value = f(value, digits))
  names(d) <- c("param", value_nm)
  
  d
}
# Rhat values for MCMC diagnostics as requested by Mazur review Oct. 2, 2024
rhat_df <- base_model$mcmc$params |>
  create_mcmc_diag()
ess_bulk_df <- base_model$mcmc$params |>
  create_mcmc_diag(fun = rstan::ess_bulk,
                   digits = 0,
                   value_nm = "ESS bulk")
ess_tail_df <- base_model$mcmc$params |>
  create_mcmc_diag(fun = rstan::ess_tail,
                   digits = 0,
                   value_nm = "ESS tail")

lu_param_nm <- tibble(param = c("ro", "h", "rbar", "rinit", "sbo",
                                "q1", "q2", "q3", "q4", "q5"),
                      Parameter = c("$R_0$", "$h$", "$\\bar{R}$", "$\\bar{R}_{init}$",
                                    "$SB_0$", "$q_1$", "$q_2$", "$q_3$", "$q_4$", "$q_5$"))
rhat_df <- rhat_df |> 
  full_join(ess_bulk_df, by = "param") |> 
  full_join(ess_tail_df, by = "param") |> 
  full_join(lu_param_nm, by = "param") |> 
  select(-param) |> 
  select(Parameter, everything())

names(rhat_df) <- c("Parameter", "$\\hat{R}$", "$ESS_{bulk}$", "$ESS_{tail}$")

ca <- "2022 stock assessment"
```

```{r removal-rate-calcs}

# This was returned from a call to find_f_b40(base_model)
# and represent the F and U values that it would take to drive the spawning biomass
# to 0.4B0 from 2022 to 2023. The catch at those rates is also included.
f_bo_40 <- list(f = c(f_fleet1 = 0.06636655,
                      f_fleet2 = 0.04168355),
                u = c(u_fleet1 = 0.06421221,
                      u_fleet2 = 0.04082674),
                catch = 4.40625)
# Difference between 0.4B0 and biomass after 50 years (kt)
diff_bio_b40 <- f(0.0064 * 1000, 1)

# extract_fleet_f <- function(d, fleet = 1){
# 
#   fleet_str <- ifelse(fleet == 1, "flt1$", "flt2$")
#   
#   d <- d |> 
#     select(catch, which(grepl(fleet_str, names(d))))
#   
#   names(d) <- gsub("_flt[1|2]$", "", names(d))
# 
#   d |> 
#     mutate(fleet = !!fleet) |> 
#     select(catch, fleet, everything())
# }
# 
# extract_sex_f <- function(d, sex = "f"){
# 
#   sex_str <- ifelse(sex == "f", "sex1$", "sex2$")
#   d <- d |> 
#     select(catch, which(grepl(sex_str, names(d))))
# 
#   names(d) <- gsub("_sex[1|2]$", "", names(d))
# 
#   out_lst <- list()
#   out_lst$fleet1 <- extract_fleet_f(d, 1) |> 
#     mutate(sex = !!sex) |> 
#     select(catch, fleet, sex, everything())
# 
#   out_lst$fleet2 <- extract_fleet_f(d, 2) |> 
#     mutate(sex = !!sex) |> 
#     select(catch, fleet, sex, everything())
#   
#   out_lst |> 
#     bind_rows()
# }

# j <- imap(basep$mcmccalcs$proj_quants, ~{
#   tib <- as_tibble(.x, rownames = "quants") |> mutate(catch = .y)
#   nms <- names(tib)
#   wch <- grep("^(F|U)20[0-9]{2}_flt[1|2]_sex[1|2]$", nms, value = T)
#   tib |>
#     select(c(catch, quants, all_of(wch))) |>
#     filter(quants == "50%") |> 
#     mutate(catch = as.numeric(catch)) |> 
#     select(-quants)
# }) |> 
#   map_df(~{.x})

# Calc probability that F < F_B40, first extract projection posteriors
# tib <- basep$mcmccalcs$proj |> 
#   bind_rows() |> 
#   rename(catch = TAC)
# nms <- names(tib)
# wch <- grep("^(F|U)20[0-9]{2}_flt[1|2]_sex[1|2]$", nms, value = T)
# j <- tib |>
#     select(c(catch, all_of(wch)))
# 
# f_lst <- list()
# f_lst$female <- extract_sex_f(j, "f")
# f_lst$male <- extract_sex_f(j, "m")
# f_df <- bind_rows(f_lst)
# 
# # Only keep 2022 F's and U's
# wch <- grep("F2022", names(f_df), value = T)
# f_ <- f_df |> 
#   select(catch, fleet, wch) |> 
#   group_by(catch, fleet) |> 
#   summarize(sum(F2022 < f_bo_40$f[1]))
# 
# wch <- grep("U2022", names(f_df), value = T)
# u_ <- f_df |> 
#   select(catch, fleet, sex, wch)

# k <- f_df |>
#   mutate(f_bo_40 = ifelse(fleet == 1, !!f_bo_40$f[["f_fleet1"]], !!f_bo_40$f[["f_fleet2"]]),
#          u_bo_40 = ifelse(fleet == 1, !!f_bo_40$u[["u_fleet1"]], !!f_bo_40$u[["u_fleet2"]])) |> 
#   mutate(F2022_F40 = F2022 / f_bo_40,
#          U2022_U40 = U2022 / u_bo_40) |> 
#   select(-c(F2022, U2022, f_bo_40, u_bo_40))
  


```

<!-- For highlighting table cells for readability. See the decision table code for example.  -->
\definecolor{faint-gray}{gray}{0.9}
